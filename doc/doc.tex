%        File: doc.tex
%     Created: tor feb 08 01:00  2018 C
% Last Change: tor feb 08 01:00  2018 C
%
\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\newcommand{\trm}[1]{\textrm{#1}}
\newcommand{\oxx}{\omega_{XX'}}
\begin{document}
\section{Shorthand notations and definitions}
\begin{table}[htpb]
  \begin{center}
    \begin{tabular}{l l}
      MC & Monte Carlo\\
      Notation & Definition\\
      FSS & Finite Size Scaling\\
      $L$ & Linear system size measured in units of lattice spacing\\
      $\beta \equiv \frac{1}{k_B T}$ & Inverse temperature\\
    \end{tabular}
  \end{center}
  \caption{Clarification of notation}
\end{table}
\section{Introduction}
The lambda phase transition in Helium-4 has been studied extensively both in theory, simulations and experiments. In 2003 a calculation of the critical exponent alpha describing the divergece of the heat capacity, was performed on the space-shuttle Something, and the exponent was calculated to the today highest standing precision. Theoretical calculations (pseudo-epsilon expansion etc) come up with a value in agreement with the experimental value. However, to date, the numerical estimates, while not extremelly far off, there still existst a discrepancy. In this thesis we have performed numerical studies to a higher degree of precision and our results show a better/the same/worse agreement with the experimental value. Agreement shows that numerical studies are a valid method of study of these systems, and discredits the idea that numberical simulations may be part of a different but closely related universality class.
\section{Method}
There are many different models in the same universality class as the normal-superfluid Helimu-4 transition, for example the $\phi^4$-model, the ddXY model etc.
In this thesis we have chosen the simulate the 3DXY-model. This is convenient because of the relative simplicity of the model, and that it is well known and studied.
The numerical method used to calculate thermodynamic averages is a Wolff algorithm.
To achieve good statistics in a reasonable amount of time, it is necessary to utilize a global update algorithm, since the model experiences so called critical slowing down near the transition temperature. 
Near the transition temperature, the correlation length diverges, so thermal fluctuations propagate very long distances, and so the system takes a very long time to equilibrate to the equilibrum value.
\subsection{Wolff Algorithm}
The Wolff algorithm is a non-local update method, and is an improvement upon the Swendsen-Wang method in which spin bonds are scanned and either deleted or frozen. This will divide the lattice into clusters, which are then flipped/updated with certain probabilites. In the Wolff algorithm only a single cluster is generated, with the starting spin uniformly random. That way of initializing the cluster, one can expect to hit large clusters with higher probability, since probability of hitting a cluster should depend on its size.
Thus one can avoid the many single spin clusters usually generated in the Swendsen-Wang method.
For a markovian update method to give a chain of system configurations with distribution according to the Boltzmann-factor, it must satisfy two conditions, (i) for any given state of the simulated system, any other state must be reachable in a finite number of steps.
(ii) Non-periodicity, it should not be possible to return to a preivous state immediatley, only after a number of steps, $t =nk,~ n = 1,2,3\dots$.
This hold for the Wolff method, since a valid cluster consists of just shifting a single spin with a random angle, thus any configuration can be reached in $N \leq N_{\trm{spins}}$ steps. Also, since the first spin selected is always flipped, the condition of Non-periodicity is also fulfilled.
The algorithm must also have a transition probability configured so that after a long time, the desired stationary distribution is reached, in our case the Boltzmann-distribution.
Call the desired staionary distribution $\rho(X)$, the transition probability $T(X \rightarrow X')$ and the probability of state $X$ at markov timestep t $\rho(X,t)$.
Then the master equaion is 
\begin{equation}
  \rho(X, t+1) - \rho(X,t) = -\sum_{X'} T(X\rightarrow X')\rho(X,t) +\sum_{X} T(X'\rightarrow X)\rho(X',t).
\end{equation}
Then solving for the stationary solution, we get
\begin{equation}
  \sum_{X'}T(X\rightarrow X') \rho(X) =  \sum_{X}T(X'\rightarrow X) \rho(X')
  \label{}
\end{equation}
with one very conveniet solution $T(X\rightarrow X') = T(X'\rightarrow X)$.
This solution can be decomposed as
\begin{equation}
  T(X\rightarrow X') = \omega_{XX'} A_{XX'}
  \label{}
\end{equation}
where $\oxx$ represents a trial probability and $A_{XX'}$ represents an acceptance probability.
We let $\omega_{XX'}$ satisfy
\begin{align}
  \oxx = \omega_{X'X}\\
  0 \leq \oxx \leq 1\\
  \sum_{X'} \oxx = 1.
  \label{}
\end{align}
Inserting this form of $T$ into the detailed balance equation then gives
\begin{equation}
  \frac{A_{XX'}}{A_{X'X}} = \frac{\rho_{X'}}{\rho_{X}}.
\end{equation}
In the metropolis algorithm one chooses the acceptance probability as follows
Step 1: From state $X$ propose a new trial state $X'$ with probability given by $\oxx$. Then accept that state with probability 
\begin{equation}
  A_{XX'} = 
  \begin{cases}
    	1 &\trm{ if } \frac{\rho(X')}{\rho(X)} \geq 1\\
	\frac{\rho(X')}{\rho(X)}&\trm{ if } \frac{\rho(X')}{\rho(X)} < 1 .
  \end{cases}
\end{equation}
The trial probability is usually just a uniform distribution.
For the Wolff-algorithm, to show that it too satisfies detailed balance, condiser a 
state $X$ and a state $X'$ differing by a flip $R(\bm{r})$ of the cluster $c$.
The transition probabilities obey
\begin{align}
  \frac{T(X\rightarrow X')}{T(X'\rightarrow X)} = \prod_{\langle s_i s_j \rangle \in \bm{\partial} c} \frac{ 1- P(R(\bm{r})\bm{s}_i\bm{s}_j))}{1 - P(R(\bm{r})\bm{s'}_i\bm{s'}_j)} = \\
  =\exp\left\{\beta \sum\limits_{\langle s_i s_j \rangle \in \bm{\partial}c }
  \label{}
\end{align}
since all probabilities inside the cluster are the same and cancel.
\section{Monte Carlo}
In statistical physics averages are calculated from 
\begin{equation}
  \langle A\rangle = \frac{1}{Z}\trm{Tr} e^{-H/T} = \sum_x A(x) P(x)
  \label{}
\end{equation}
where  $P(x) = (1/Z)exp(-H(x))$ is the Boltzmann distribution.
The practical and convievable way to evaluate these averages using Monte Carlo is to use importance sampling, i.e. do not form averages from uniformly random system configurations, but instead generate configurations that are Boltzmann distributed.   
Thus thermal Monte Carlo averages have the form 
\begin{equation}
  \langle A \rangle = \frac{1}{N} \sum_x A(x) \pm \frac{\sigma	}{\sqrt N}
  \label{}
\end{equation}
where the states $x$ are Boltzmann distributed.
However, the Boltzmann distribution may be unsuitable for some systems/configurations we want to study, so we can use any other distribution as 
\begin{equation}
  \langle A \rangle = \frac{\frac{1}{N}\sum_y \frac{A(y) e^{-H(x)/T}}{P'(y)}}{\frac{1}{N}\sum_y \frac{e^{-H(y)/T}}{P'(y)}}  
  \label{}
\end{equation}
 where the states $y$ are distributed according to $P'(y)$.

 Importance sampling is a way of reducing the error in the Monte Carlo estimate. Instead of choosing sample points distributed uniformly, one can try to choose the most probable states. Since thermal systems are Boltzmann-distributed, we want a way to randomly generate states distributed according to the Boltzmann distribution.
 \section{Markov process}
 A Markovian process is one for which the probility of getting to any state in the system is determined solely by the current state. I.e. the probability of going from state $x_i$ to $x_{j}$ is given by a transition probability $w(x_i \rightarrow x_j)$. The master equation describing the evolution of the probability distribution can then be written $P(x_i )$
 
\section{Detailed Balance}
\section{Wolff Algorithm}


\section{Histrogram extrapolation}
Histogram extrapolation is a method to extract more data from a run. 
Averages are calculated as
\begin{equation}
  \langle A \rangle_{\beta_0} =  \frac{\sum_{x}A_x e^{-\beta_0 H_x}}{\sum_{x}e^{-\beta_0 H_x}}
  \label{}
\end{equation}
To get the average of A at some different temperature, say $\beta_1$, we can do
\begin{align}
  \langle A\rangle_{\beta_1} =  \frac{\sum_{x}A_x e^{-\beta_1 H_x}}{\sum_{x}e^{-\beta_1 H_x}} = \\
  = \frac{\sum_x A_x e^{-(\beta_1 - \beta_0)H_x} e^{-\beta_0 H_x}}{\sum_x e^{-(\beta_1 - \beta_0)H_x}e^{-\beta_0 H_x}} = \\
  = \frac{\langle A e^{-(\beta_1 - \beta_0)H}\rangle_{\beta_0}}{\langle e^{-(\beta_1 - \beta_0)H}\rangle_{\beta_0}}
\end{align}
Since the value of $abs(\beta_1 -\beta_0)$ will be small in practice, and the value of the energy will increase with the systemsize, precision will be lost in the calculation of $\exp\left( \beta_1 - \beta_0 \right)H_x$. To mitigate this, one can subtract a constant from the energy of the system, $H_max$, since a constant shift in energy will not change expectation values, since it just factors out of both sums and cancels.
\section{Binder cumulant}
The quantity $b =\frac{\langle M^4 \rangle}{\langle M^2\rangle^2}$ is useful in determining the critical temperature, as it in the thermodynamic limit $L\rightarrow \infty $ essentially becomes a heaviside function, discontinous at the critical temperature. 
\section{Definitions of scaling variables}
\begin{align}
  \beta &= \frac{1}{k_B T} \\
  F &= -\beta \log Z\\
  F_s(u_t,u_h) &= L^{-d}f_s(L^{y_t}u_t,L^{y_h}u_h)\\
  M &\sim (-t)^{\beta}\\
  \chi &\sim |t|^{-\gamma}\\
  \xi &\sim |t|^{-\nu}
\end{align}  
\section{Hasenbusch results}
\begin{table}[htpb]
\begin{center}
\begin{tabular}{l l l}
  Quantity & Hasenbusch & Our\\
  $T_c$	& 2.20184(6) & N/A\\
  $\alpha $	& -0.0151(3) & N/A\\
  $\nu	  $      & 0.6717(1) & N/A\\
  $\eta   $      & 0.0381(2) & N/A\\
  $\gamma $	& 1.3178(2) & N/A\\
  $\delta $	& 4.780(1) & N/A\\
  $\beta  $      & 0.3486(1) & N/A\\
  $\omega $      & 0.785(20) & N/A\\
  $\omega_2$ 	& 1.8(2) & N/A\\
\end{tabular}
\end{center}
\caption{3DXY-model }
\end{table}
\begin{table}[htpb]
\begin{center}
\begin{tabular}{l l l}
  Quantity  & Hasenbusch & Our\\
  $T_c$	    & 4.511523(3)& N/A\\
  $\alpha $ & N/A        & N/A\\
  $\nu	  $ & 0.63002(10)& N/A\\
  $\eta   $ & 0.03627(10)& N/A\\
  $\gamma $ & N/A        & N/A\\
  $\delta $ & N/A        & N/A\\
  $\beta  $ & N/A        & N/A\\
  $\omega $ & N/A        & N/A\\
  $\omega_2$& N/A        & N/A\\
\end{tabular}
\end{center}
\caption{Ising3D-model }
\end{table}

\section{Finite size scaling}

Assuming higher order corrections are irrelevant, the FSS(finite size scaling) of the  4th order Binder cumulant at the critical temperature is
\begin{equation}
  B(L,t=0) = b_0 + b_1L^{-\omega}.
\end{equation}
This means that
\begin{align}
  b'(L) \equiv B(2L,t=0) - B(L,t=0) &= b_1(2^{-\omega} -1)L^{-\omega}\\
  b''(L)\equiv \frac{b'(2L)}{b'(L)} &= 2^{-\omega}\\
  \omega &= -\frac{\ln(b''(L))}{\ln(2)}
\end{align}
Note that this only holds at the critical temperature, the $\textit{constants}$ $b_0, b_2$ are functions of temperature. Thus if we plot this function using at least four different system sizes, we can get several graphs that intersect at the critical temperature, and from that the scaling correction $\omega$ can be read of without the need for any form of parameter fitting. This depends on the assumption that the higher order corrections are sufficiently small, which they might not in fact be for the lowest system size we have simulated, $L=4$. This method also fails when $b'(L)$ and $b'(2L)$ differ in sign, since then $log(b''(l)$ will be undefined.
For such cases one can resort to methods of parameter fitting.
Consider the Superfluid density, which scales as 
\begin{align}
  r(L)&\equiv \rho_s(L,t=0) \cdot L = r_0 + r_1L^{-\omega_\rho}\\
  r(2L) - r(L) &= r_1(2^{-\omega_\rho} -1)L^{-\omega_\rho}
  \label{}
\end{align}
One very convenient solution is the detailed balance solution,
$T(X\rightarrow X') = T(X' \rightarrow X)$.
This solution can be decomposed into $T(X\rightarrow X') = \omega_{XX'} A_{XX'}$ where we demand that $\omega{XX'}$ be symmetric.


For the method using 2 systemsizes, one can check by visual inspeciton each different value of omega to see for which omega the graphs intersect mostly at one point. One can also make a numerical check, by finding the intersecitons of all curves, and calculating  and plotting the standard deviation of the intersections for each omega.

\section{Results}
\subsection{Data Analysis}
The simulation program outputs raw data ordered in lines. Data from one ``simulation'' is contained in $N_{temperatures} \cdot N_{averages per simulation} \cdot N_{simulations}$.
To calculate quantities of interest from this raw data, a python3 script has been written, utilizing the numpy library. All available raw data is loaded into a numpy ndarray, and sorted by system size and temperature. Then for each block of data with one temperature and one system size, combined averages are calculated and from these, further functions of the averages are calculated, such as the Binder Parameter, it's derivative, Susceptibility, Superfluid density, etc. To estimate the statistical error in these estimations, we utilize a resampling method usually refered to as the jackknife method. For each block used in calculating the estimations, a jackknife function is called, which calculates the quantities again, but with a subblock of data omitted.
\subsection{3DXY-model}
Simulations were performed using the Wolff-cluster algorithm at the temperature $T_{run} = 2.2020~K$. Histogram extrapolation was performed to extrapolate to the range of temperatures $T_{range} = 2.2015-2.2030 ~ K$.
The simulations are structured so that a lattice is initialized in the zero temperature state, all spins pointing in one direction. Then clusterupdates are performed until over 100k sweeps have been performed.

(We say that one sweep has been performed when a number of spins equal to the total number of spins on the lattice has been tested to be added to a cluster.
Thus the number of clusters and sweeps performed as warmup varies slightly between independent simulations)

After the warmup, cluster updates are performed for 100k more sweeps, and the thermodynamic quantities are collected after each update. Then, averages are calculated and printed. Then 100 sweeps are performed to take the lattice to a state not correlated, and another 100k sample sweeps are performed. This is repeated 100 times. The system sizes simulated are 4, 8, 16, 32, 64 and 128. 
The number of simulations currently used to calculate $U_4, \rho_s, \chi $ etc. for the system sizes 4

\begin{table}[htpb]
\begin{center}
\begin{tabular}{l l l}
  L & No. simulations & No. uncorrelated averages\\
  4 & 110 & 11000\\
  8 & 110 & 11000\\
  16 & 45.54 & 4554\\
  32 & 12 & 1200\\
  64 & 1.16 & 116 \\
  128 & 0  & 0 \\
\end{tabular}
\end{center}
\caption{Number of simulations performed on the 3DXY-model}
\end{table}

\begin{table}[htpb]
\begin{center}
\begin{tabular}{l l l}
  L & No. simulations & No. uncorrelated averages\\
  4 & 100 & 10000\\
  8 & 100 & 10000\\
  16 & 100 & 10000\\
  32 & 97.52 & 9752 \\
  64 & 5.8 & 580 \\
  128 & 1.15  & $115^{*}$ \\
\end{tabular}
\end{center}
\caption{Number of simulations performed on the Ising3D-model  \textsuperscript{*The simulation of 128 is not doing so well, as evident in the plots}}
\end{table}
\subsection{Ising3D model}
The same general formula is used when simulating the Ising3D model, however, the temperature range is shifted to capture the phasetransition, $T_{\trm{run}} = 4.50, ~ T_{\trm{range}} = 4.486-4.515$.



\input{figures.tex}

\end{document}
