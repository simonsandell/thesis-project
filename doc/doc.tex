%        File: doc.tex
%     Created: tor feb 08 01:00  2018 C
% Last Change: tor feb 08 01:00  2018 C
%
\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\newcommand{\trm}[1]{\textrm{#1}}
\begin{document}
\section{Shorthand notations and definitions}
\begin{table}[htpb]
  \begin{center}
    \begin{tabular}{l l}
      MC & Monte Carlo\\
      Notation & Definition\\
      FSS & Finite Size Scaling\\
      $L$ & Linear system size measured in units of lattice spacing\\
      $\beta \equiv \frac{1}{k_B T}$ & Inverse temperature\\
    \end{tabular}
  \end{center}
  \caption{Clarification of notation}
\end{table}
\section{Introduction}
The lambda phase transition in Helium-4 has been studied extensively both in theory, simulations and experiments. In 2003 a calculation of the critical exponent alpha describing the divergece of the heat capacity, was performed on the space-shuttle Something, and the exponent was calculated to the today highest standing precision. Theoretical calculations (pseudo-epsilon expansion etc) come up with a value in agreement with the experimental value. However, to date, the numerical estimates, while not extremelly far off, there still existst a discrepancy. In this thesis we have performed numerical studies to a higher degree of precision and our results show a better/the same/worse agreement with the experimental value. Agreement shows that numerical studies are a valid method of study of these systems, and discredits the idea that numberical simulations may be part of a different but closely related universality class.
\section{Method}
There are many different models in the same universality class as the normal-superfluid Helimu-4 transition, for example \dots.
In this thesis we have chosen the simulate the 3DXY-model. This is convenient because of the relative simplicity of the model, and that it is well known and studied.
The numerical method used to calculate thermodynamic averages is a Wolff algorithm.
To achieve good statistics in a reasonable amount of time, it is necessary to utilize a global update algorithm, since the model experiences so called critical slowing down near the transition temperature. 
Near the transition temperature, the correlation length diverges, so thermal fluctuations propagate very long distances, and so the system takes a very long time to equilibrate to the equilibrum value.
\subsection{Wolff Algorithm}

\section{Monte Carlo}
In statistical physics averages are calculated from 
\begin{equation}
  \langle A\rangle = \frac{1}{Z}\trm{Tr} e^{-H/T} = \sum_x A(x) P(x)
  \label{}
\end{equation}
where  $P(x) = (1/Z)exp(-H(x))$ is the Boltzmann distribution.
The practical and convievable way to evaluate these averages using Monte Carlo is to use importance sampling, i.e. do not form averages from uniformly random system configurations, but instead generate configurations that are Boltzmann distributed.   
Thus thermal Monte Carlo averages have the form 
\begin{equation}
  \langle A \rangle = \frac{1}{N} \sum_x A(x) \pm \frac{\sigma	}{\sqrt N}
  \label{}
\end{equation}
where the states $x$ are Boltzmann distributed.
However, the Boltzmann distribution may be unsuitable for some systems/configurations we want to study, so we can use any other distribution as 
\begin{equation}
  \langle A \rangle = \frac{\frac{1}{N}\sum_y \frac{A(y) e^{-H(x)/T}}{P'(y)}}{\frac{1}{N}\sum_y \frac{e^{-H(y)/T}}{P'(y)}}  
  \label{}
\end{equation}
 where the states $y$ are distributed according to $P'(y)$.

 Importance sampling is a way of reducing the error in the Monte Carlo estimate. Instead of choosing sample points distributed uniformly, one can try to choose the most probable states. Since thermal systems are Boltzmann-distributed, we want a way to randomly generate states distributed according to the Boltzmann distribution.
 \section{Markov process}
 A Markovian process is one for which the probility of getting to any state in the system is determined solely by the current state. I.e. the probability of going from state $x_i$ to $x_{j}$ is given by a transition probability $w(x_i \rightarrow x_j)$. The master equation describing the evolution of the probability distribution can then be written $P(x_i )$
 
\section{Detailed Balance}
\section{Wolff Algorithm}


\section{Histrogram extrapolation}
Histogram extrapolation is a method to extract more data from a run. 
Averages are calculated as
\begin{equation}
  \langle A \rangle_{\beta_0} =  \frac{\sum_{x}A_x e^{-\beta_0 H_x}}{\sum_{x}e^{-\beta_0 H_x}}
  \label{}
\end{equation}
To get the average of A at some different temperature, say $\beta_1$, we can do
\begin{align}
  \langle A\rangle_{\beta_1} =  \frac{\sum_{x}A_x e^{-\beta_1 H_x}}{\sum_{x}e^{-\beta_1 H_x}} = \\
  = \frac{\sum_x A_x e^{-(\beta_1 - \beta_0)H_x} e^{-\beta_0 H_x}}{\sum_x e^{-(\beta_1 - \beta_0)H_x}e^{-\beta_0 H_x}} = \\
  = \frac{\langle A e^{-(\beta_1 - \beta_0)H}\rangle_{\beta_0}}{\langle e^{-(\beta_1 - \beta_0)H}\rangle_{\beta_0}}
\end{align}
Since the value of $abs(\beta_1 -\beta_0)$ will be small in practice, and the value of the energy will increase with the systemsize, precision will be lost in the calculation of $\exp\left( \beta_1 - \beta_0 \right)H_x$. To mitigate this, one can subtract a constant from the energy of the system, $H_max$, since a constant shift in energy will not change expectation values, since it just factors out of both sums and cancels.
\section{Binder cumulant}
The quantity $b =\frac{\langle M^4 \rangle}{\langle M^2\rangle^2}$ is useful in determining the critical temperature, as it in the thermodynamic limit $L\rightarrow \infty $ essentially becomes a heaviside function, discontinous at the critical temperature. 
\section{Definitions of scaling variables}
\begin{align}
  \beta &= \frac{1}{k_B T} \\
  F &= -\beta \log Z\\
  F_s(u_t,u_h) &= L^{-d}f_s(L^{y_t}u_t,L^{y_h}u_h)\\
  M &\sim (-t)^{\beta}\\
  \chi &\sim |t|^{-\gamma}\\
  \xi &\sim |t|^{-\nu}
\end{align}  
\section{Hasenbusch results}
\begin{table}[htpb]
\begin{center}
\begin{tabular}{l l l}
  Quantity & Hasenbusch & Our\\
  $T_c$	& 2.20184(6) & N/A\\
  $\alpha $	& -0.0151(3) & N/A\\
  $\nu	  $      & 0.6717(1) & N/A\\
  $\eta   $      & 0.0381(2) & N/A\\
  $\gamma $	& 1.3178(2) & N/A\\
  $\delta $	& 4.780(1) & N/A\\
  $\beta  $      & 0.3486(1) & N/A\\
  $\omega $      & 0.785(20) & N/A\\
  $\omega_2$ 	& 1.8(2) & N/A\\
\end{tabular}
\end{center}
\caption{3DXY-model }
\end{table}

\section{Finite size scaling}

Assuming higher order corrections are irrelevant, the FSS(finite size scaling) of the  4th order Binder cumulant at the critical temperature is
\begin{equation}
  B(L,t=0) = b_0 + b_1L^{-\omega}.
\end{equation}
This means that
\begin{align}
  b'(L) \equiv B(2L,t=0) - B(L,t=0) &= b_1(2^{-\omega} -1)L^{-\omega}\\
  b''(L)\equiv \frac{b'(2L)}{b'(L)} &= 2^{-\omega}\\
  \omega &= -\frac{\ln(b''(L))}{\ln(2)}
\end{align}
Note that this only holds at the critical temperature, the $\textit{constants}$ $b_0, b_2$ are functions of temperature. Thus if we plot this function using at least four different system sizes, we can get several graphs that intersect at the critical temperature, and from that the scaling correction $\omega$ can be read of without the need for any form of parameter fitting. This depends on the assumption that the higher order corrections are sufficiently small, which they might not in fact be for the lowest system size we have simulated, $L=4$. This method also fails when $b'(L)$ and $b'(2L)$ differ in sign, since then $log(b''(l)$ will be undefined.
For such cases one can resort to methods of parameter fitting.
Consider the Superfluid density, which scales as 
\begin{align}
  r(L)&\equiv \rho_s(L,t=0) \cdot L = r_0 + r_1L^{-\omega_\rho}\\
  r(2L) - r(L) &= r_1(2^{-\omega_\rho} -1)L^{-\omega_\rho}
  \label{<++>}
\end{align}<++>
\section{Results}
\subsection{3DXY-model}
Simulations were performed using the Wolff-cluster algorithm at the temperature $T_{run} = 2.2020~K$. Histogram extrapolation was performed to extrapolate to the range of temperatures $T_{range} = 2.2015-2.2030 ~ K$.
The simulations are structured so that a lattice is initialized in the zero temperature state, all spins pointing in one direction. Then clusterupdates are performed until over 100k sweeps have been performed.

(We say that one sweep has been performed when a number of spins equal to the total number of spins on the lattice has been tested to be added to a cluster.
Thus the number of clusters and sweeps performed as warmup varies slightly between independent simulations)

After the warmup, cluster updates are performed for 100k more sweeps, and the thermodynamic quantities are collected after each update. Then, averages are calculated and printed. Then 100 sweeps are performed to take the lattice to a state not correlated, and another 100k sample sweeps are performed. This is repeated 100 times. The system sizes simulated are 4, 8, 16, 32, 64 and 128. 
The number of simulations currently used to calculate $U_4, \rho_s, \chi $ etc. for the system sizes 4

\begin{table}[htpb]
\begin{center}
\begin{tabular}{l l }
  L & No. simulations\\
  4 & 110 \\
  8 & 28.4 \\
  16 & 18.34 \\
  32 & 4.49 \\
  64 & 0.34 \\
  128 & 0 \\
\end{tabular}
\end{center}
\caption{Number of simulations performed on the 3DXY-model}
\end{table}

\input{figures.tex}

\end{document}
