%        File: preface.tex
%     Created: mån feb 12 01:00  2018 C
% Last Change: mån feb 12 01:00  2018 C
%

\section{Monte Carlo}
In statistical physics, predictions come from thermal averages calculated from 
\begin{equation}
  \langle A\rangle = \frac{1}{Z}\trm{Tr} e^{-H/T} = \sum_x A(x) P(x)
\end{equation}
where  $P(x) = (1/Z)\trm{exp}(-H(x))$ is the Boltzmann distribution.
Models of physical systems are described by writing down the mathematical expression for the Hamiltonian. However, an expression for the partition function $Z$ can in most cases not be found. So using the first representation to calculate averages is for most models not possible. The other form shows another way to calculate the averages, at least approximatesly, by simulation. 
Monte Carlo averages have the form 
\begin{equation}
  \langle A \rangle = \frac{1}{N} \sum_x A(x) \pm \frac{\sigma	}{\sqrt N}
\end{equation}
where the states $x$ are Boltzmann distributed.

The Boltzmann distribution may be unsuitable for some systems/configurations we want to study. In practice, we can use any distribution we want, if we introduce the proper normalization.
\begin{equation}
  \langle A \rangle = \frac{\frac{1}{N}\sum_y \frac{A(y) e^{-H(x)/T}}{P'(y)}}{\frac{1}{N}\sum_y \frac{e^{-H(y)/T}}{P'(y)}}  
\end{equation}
 where the states $y$ are distributed according to $P'(y)$.

%Importance sampling is a way of reducing the error in the Monte Carlo estimate. Instead of choosing sample points distributed uniformly, one can try to choose the most probable states. Since thermal systems are Boltzmann-distributed, we want a way to randomly generate states distributed according to the Boltzmann distribution.

%//\section{Markov process}
%//A Markovian process is one for which the probility of getting to any state in the system is determined solely by the current state. I.e. the probability of going from state $x_i$ to $x_{j}$ is given by a transition probability $w(x_i \rightarrow x_j)$. The master equation describing the evolution of the probability distribution can then be written $P(x_{i+1}) = k
%// 
%//

\section{Histrogram extrapolation}
Histogram extrapolation is a method used to extrapolate data obtained from a simualtion.
By calculating additional averages during the simulation proceeds, one can obtain data for a range of temperatures surrounding the actual temperature the simulation is running at.
The averages are calculated as
\begin{equation}
  \langle A \rangle_{\beta_0} =  \frac{\sum_{x}A_x e^{-\beta_0 H_x}}{\sum_{x}e^{-\beta_0 H_x}}
\end{equation}
To get the average of A at some different temperature, say $\beta_1$, we can do
\begin{align}
  \langle A\rangle_{\beta_1} =  \frac{\sum_{x}A_x e^{-\beta_1 H_x}}{\sum_{x}e^{-\beta_1 H_x}} = \\
  = \frac{\sum_x A_x e^{-(\beta_1 - \beta_0)H_x} e^{-\beta_0 H_x}}{\sum_x e^{-(\beta_1 - \beta_0)H_x}e^{-\beta_0 H_x}} = \\
  = \frac{\langle A e^{-(\beta_1 - \beta_0)H}\rangle_{\beta_0}}{\langle e^{-(\beta_1 - \beta_0)H}\rangle_{\beta_0}}.
\end{align}
The range of temperatures to which extrapolation is viable will decrease with system size,
as loss in precision will occur when the exponential factor becomes large. To minimize the exponential factor, one can shift the energy of the system by a constant, which we call $H_{max}$. A constant shift in energy does not affect the values, 
\begin{equation}
  \langle A \rangle = \frac{e^{\beta H_{max}}}{e^{\beta H_{max}}}\frac{\sum_x A_x e^{-\beta H_x}}{\sum_x e^{-\beta H_x}}= \frac{\sum_x A_x e^{-\beta(H_x-H_{max})}}{\sum_x e^{-\beta(H_x-H_{max})}}.
  \label{}
\end{equation}
This technique can extend the range of temperate, but fluctuations in energy grow with the system size, so for high enough system sizes, extrapolation becomes fruitless.

\section{Wolff Algorithm}
The Wolff algorithm is a non-local update method, and is an improvement upon the Swendsen-Wang method in which spin bonds are scanned and either deleted or frozen. This will divide the lattice into clusters, which are then flipped/updated with certain probabilites. In the Wolff algorithm only a single cluster is generated, with the starting spin uniformly random. That way of initializing the cluster, one can expect to hit large clusters with higher probability, since probability of hitting a cluster should depend on its size.
Thus one can avoid the many single spin clusters usually generated in the Swendsen-Wang method.
For a markovian update method to give a chain of system configurations with distribution according to the Boltzmann-factor, it must satisfy two conditions, 

(i) for any given state of the simulated system, any other state must be reachable in a finite number of steps.

(ii) Non-periodicity, it should not be possible to return to a preivous state immediatley, only after a number of steps, $t =nk,~ n = 1,2,3\dots$.
This hold for the Wolff method, since a valid cluster consists of just shifting a single spin with a random angle, thus any configuration can be reached in $N \leq N_{\trm{spins}}$ steps. Also, since the first spin selected is always flipped, the condition of Non-periodicity is also fulfilled.
The algorithm must also have a transition probability configured so that after a long time, the desired stationary distribution is reached, in our case the Boltzmann-distribution.
Call the desired staionary distribution $\rho(X)$, the transition probability $T(X \rightarrow X')$ and the probability of state $X$ at markov timestep t $\rho(X,t)$.
Then the master equaion is 
\begin{equation}
  \rho(X, t+1) - \rho(X,t) = -\sum_{X'} T(X\rightarrow X')\rho(X,t) +\sum_{X} T(X'\rightarrow X)\rho(X',t).
\end{equation}
Then solving for the stationary solution, we get
\begin{equation}
  \sum_{X'}T(X\rightarrow X') \rho(X) =  \sum_{X}T(X'\rightarrow X) \rho(X')
\end{equation}
which is hard to solve in general, but with a very famous paritcular solution called the detailed balace solution can be found,
\begin{align}
  \frac{T(X\rightarrow X')}{T(X'\rightarrow X)} = \frac{\rho(X')}{\rho(X)} 
  \label{eq:detbal}
\end{align}
for all states $X,~X'$.

This solution can be decomposed as
\begin{equation}
  T(X\rightarrow X') = \omega_{XX'} A_{XX'}
\end{equation}
where $\oxx$ represents a trial probability and $A_{XX'}$ represents an acceptance probability.
We let $\omega_{XX'}$ satisfy
\begin{align}
  \oxx = \omega_{X'X}\\
  0 \leq \oxx \leq 1\\
  \sum_{X'} \oxx = 1.
\end{align}
Inserting this form of $T$ into the detailed balance equation then gives
\begin{equation}
  \frac{A_{XX'}}{A_{X'X}} = \frac{\rho_{X'}}{\rho_{X}}.
\end{equation}
In the metropolis algorithm one chooses the acceptance probability as follows
Step 1: From state $X$ propose a new trial state $X'$ with probability given by $\oxx$. Then accept that state with probability 
\begin{equation}
  A_{XX'} = 
  \begin{cases}
    	1 &\trm{ if } \frac{\rho(X')}{\rho(X)} \geq 1\\
	\frac{\rho(X')}{\rho(X)}&\trm{ if } \frac{\rho(X')}{\rho(X)} < 1 .
  \end{cases}
\end{equation}
The trial probability is usually just a uniform distribution.
\section{Wolff on the 3DXY-model}
In the 3DXY-model, each spin is characterized by a single value, their angle $\alpha$.
We define the hamiltonian of the 3DXY-model as 
\begin{equation}
  H = \beta\sum_{<s_i,s_j>} \cos(\alpha_i - \alpha_j)
\end{equation}
where $<s_i,s_>$ denote that only sites $s_i$ and $s_j$ which are nearest neighours should be included.
The probability of adding spin to the cluster is
\begin{equation}
 P(\alpha_i,\alpha_j,\alpha_u) = 1 - \exp(2\beta \cos(\alpha_j - \alpha_u)\cos(\alpha_i - \alpha_u))
  \label{pflip}
\end{equation}
where $\bm{u}$ is the normal to the plane through which the spins are reflected when added.
When a spin is added to the cluster, it is reflected through a plane.
The Wolff-algorithm is defined as follows for the 3DXY-model:
\begin{enumerate}[(i)]
\item Select a starting spin with uniform probability. Select a random plane with normal $\bm{u}$, with the angle $\alpha_u$  to reflect spins through. Reflect the starting spin through the plane, taking it to 
$$\alpha \rightarrow R(\alpha,\alpha_u) = \pi + 2\alpha_u - \alpha$$

and mark it as part of cluster.

\item Add it's neighbours to a list of perimeter spins, to be tried for inclusion in the cluster.

\item Pick out any element in the list. Try to add it to the cluster with probability as defined above. If it is accepted, add it to the cluster. Add nearest neighbours that are not part of the cluster already to the perimeter list.

\item Repeat (iii) until the perimeter list is empty.
\end{enumerate}
For the Wolff-algorithm, to show that it satisfies detailed balance, condiser a 
state $X$ and a state $X'$ differing by a flip of the cluster $c$. 
The transition probabilities obey
\begin{align}
  \frac{T(X\rightarrow X')}{T(X'\rightarrow X)} &= \prod_{\langle s_i s_j \rangle \in \bm{\partial} c} \frac{ 1- P(R(\alpha_i,\alpha_u),\alpha_j,\alpha_u)}{1- P(R(\alpha'_i,\alpha_u),\alpha'_j,\alpha_u)}
\end{align}
where the product is over nearest neighbour bonds where $s_i \in x, s_j \notin c$.
Thus we have that $ R(\alpha_i,\alpha_u) = \alpha'_i $, $R(\alpha'_i) = \alpha_i$ and $\alpha_j = \alpha'_j$.
We get
\begin{align}
  \frac{T(X\rightarrow X')}{T(X'\rightarrow X)}&= \prod_{\langle s_i s_j \rangle \in \bm{\partial} c} \frac{ \exp\left\{2\beta\cos(R(\alpha_i,\alpha_u) - \alpha_u)\cos(\alpha_j - \alpha_u)\right\}}{ \exp\left\{2\beta\cos(R(\alpha'_i,\alpha_u) - \alpha_u)\cos(\alpha'_j -\alpha_u)\right\}} = \\
  &= \prod_{\langle s_i s_j \rangle \in \bm{\partial} c} \frac{ \exp\left\{2\beta\cos(\alpha'_i - \alpha_u)\cos(\alpha'_j - \alpha_u)\right\}}{ \exp\left\{2\beta\cos(\alpha_i,\alpha_u) - \alpha_u)\cos(\alpha_j -\alpha_u)\right\}} = \\
  &= \frac{\rho(X')}{\rho(X)},
\end{align}
and so, detailed balance is satisfied.

