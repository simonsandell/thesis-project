%        File: preface.tex
%     Created: mån feb 12 01:00  2018 C
% Last Change: mån feb 12 01:00  2018 C
%
% TODO: expand on monte carlo method. write about necessity of quality RNG's  
\section{Monte Carlo}
In the theoretical framework of statistical physics, physical predictions come from thermal averages, calculated from 
\begin{equation}
  \langle A\rangle = \frac{1}{Z}\trm{Tr} e^{-H/T} = \sum_x A(x) P(x)
  \label{MC:eq:avg}
\end{equation}
where  $P(x) = (1/Z)\trm{exp}(-H(x))$ is the Boltzmann distribution, and the sum is a sum over all possible states of the system. 
Models of physical systems are described by writing down the mathematical expression for the Hamiltonian.
An expression for the partition funciton $Z$ can be written down, but for most models, the number of states are infinite or at least impractically large, so the first formula above can seldom be used in practice to calculate the averages.
Estimation of such very large integrals can be done in several ways.
One way is to calculate the value of the function to be integrated at equidistant points inside the integration area and extrapolating to obtain a measure of the full integral.
This method can be biased, if you do not know your function with full detail, you may select points that are somehow correlated or otherwise introduces an unnecessary error.
A better and safer way to obtain an estimate of the integral is to use randomly distributed points.
This will remove potential bias and will often yeild a better estimate when using the same number of points.

In the field of statistical physics, . By simulating a model, and updating the state of the model randomly, but with such rules so that it's states are Boltzmann-distributed, one can estimate the thermal averages by calculating the corresponding quantity of the simulated model at different 'time-steps'.

When implementing such an algorithm, it is convenient to use a pseudo-random number generator (pRNG) to produce random numbers.
The quality of the pRNG is very important when implementing a Monte Carlo-method.
Since one often wants to generate a large number of random numbers, it is important that the generator produces numbers that are not correlated to the previous ones, as this could lead to bias. 
In this work we use a pRNG implemented in the standard library of C++, the mt\_19937\_64.

%The Boltzmann distribution may be unsuitable for some systems/configurations we want to study. In practice, we can use any distribution we want, if we introduce the proper normalization.
%\begin{equation}
 % \langle A \rangle = \frac{\frac{1}{N}\sum_y \frac{A(y) e^{-H(x)/T}}{P'(y)}}{\frac{1}{N}\sum_y \frac{e^{-H(y)/T}}{P'(y)}}  
%\end{equation}
 %where the states $y$ are distributed according to $P'(y)$.

%Importance sampling is a way of reducing the error in the Monte Carlo estimate. Instead of choosing sample points distributed uniformly, one can try to choose the most probable states. Since thermal systems are Boltzmann-distributed, we want a way to randomly generate states distributed according to the Boltzmann distribution.

%//\section{Markov process}
%//A Markovian process is one for which the probility of getting to any state in the system is determined solely by the current state. I.e. the probability of going from state $x_i$ to $x_{j}$ is given by a transition probability $w(x_i \rightarrow x_j)$. The master equation describing the evolution of the probability distribution can then be written $P(x_{i+1}) = k
%// 
%//

\section{Histrogram extrapolation}
Histogram extrapolation is a method used to extrapolate data obtained from a simualtion.
By calculating additional averages during the simulation proceeds, one can obtain data for a range of temperatures surrounding the temperature the simulation is running at.
The averages are calculated as
\begin{equation}
  \langle A \rangle_{\beta_0} =  \frac{\sum_{x}A_x e^{-\beta_0 H_x}}{\sum_{x}e^{-\beta_0 H_x}}
\end{equation}
To get the average of A at some different temperature, say $\beta_1$, we can do
\begin{equation}
  \langle A \rangle_{\beta_1} =  \frac{\sum_{x}A_x e^{-\beta_1 H_x}}{\sum_{x}e^{- \beta_1 H_x}} = \\
  = \frac{\sum_x A_x e^{-(\beta_1 - \beta_0)H_x} e^{-\beta_0 H_x}}{\sum_x e^{-(\beta_1 - \beta_0)H_x}e^{-\beta_0 H_x}} = \\
  = \frac{\langle A e^{-(\beta_1 - \beta_0)H}\rangle_{\beta_0}}{\langle e^{-(\beta_1 - \beta_0)H}\rangle_{\beta_0}}.
\end{equation}
The range of temperatures to which extrapolation is viable will decrease with system size,
as loss in precision will occur when the exponential factor becomes large. To minimize the exponential factor, one can shift the energy of the system by a constant, which we call $H_{max}$. A constant shift in energy does not affect the values, 
\begin{equation}
  \langle A \rangle = \frac{e^{\beta H_{max}}}{e^{\beta H_{max}}}\frac{\sum_x A_x e^{-\beta H_x}}{\sum_x e^{-\beta H_x}}= \frac{\sum_x A_x e^{-\beta(H_x-H_{max})}}{\sum_x e^{-\beta(H_x-H_{max})}}.
  \label{}
\end{equation}
This technique can extend the range of temperate, but fluctuations in energy grow with the system size, so for high enough system sizes, extrapolation becomes fruitless.
The data gathered at the extrapolated temperature will be worse and worse the further we go, since we generate states that are probable for actual running temperature, but these states will likely become more and more unimportant for the extrapolated temperature.
\section{Wolff Algorithm}
The Wolff algorithm is a non-local update method, and is an improvement upon the Swendsen-Wang method in which spin bonds are scanned and either deleted or frozen. This will divide the lattice into clusters, which are then flipped/updated with certain probabilites. In the Wolff algorithm only a single cluster is generated, with the starting spin uniformly random. That way of initializing the cluster, one can expect to hit large clusters with higher probability, since probability of hitting a cluster should depend on its size.
Thus one can avoid the many single spin clusters usually generated in the Swendsen-Wang method.
For a markovian update method to give a chain of system configurations with distribution according to the Boltzmann-factor, it must satisfy two conditions, 

(i) for any given state of the simulated system, any other state must be reachable in a finite number of steps.

(ii) Non-periodicity, it should not be possible to return to a preivous state immediatley, only after a number of steps, $t =nk,~ n = 1,2,3\dots$.
This hold for the Wolff method, since a valid cluster consists of just shifting a single spin with a random angle, thus any configuration can be reached in $N \leq N_{\trm{spins}}$ steps. Also, since the first spin selected is always flipped, the condition of Non-periodicity is also fulfilled.
The algorithm must also have a transition probability configured so that after a long time, the desired stationary distribution is reached, in our case the Boltzmann-distribution.
Call the desired staionary distribution $\rho(X)$, the transition probability $T(X \rightarrow X')$ and the probability of state $X$ at markov timestep t $\rho(X,t)$.
Then the master equaion is 
\begin{equation}
  \rho(X, t+1) - \rho(X,t) = -\sum_{X'} T(X\rightarrow X')\rho(X,t) +\sum_{X} T(X'\rightarrow X)\rho(X',t).
\end{equation}
Then solving for the stationary solution, we get
\begin{equation}
  \sum_{X'}T(X\rightarrow X') \rho(X) =  \sum_{X}T(X'\rightarrow X) \rho(X')
\end{equation}
which is hard to solve in general, but with a very famous paritcular solution called the detailed balace solution can be found,
\begin{align}
  \frac{T(X\rightarrow X')}{T(X'\rightarrow X)} = \frac{\rho(X')}{\rho(X)} 
  \label{eq:detbal}
\end{align}
for all states $X,~X'$.

This solution can be decomposed as
\begin{equation}
  T(X\rightarrow X') = \omega_{XX'} A_{XX'}
\end{equation}
where $\oxx$ represents a trial probability and $A_{XX'}$ represents an acceptance probability.
We let $\omega_{XX'}$ satisfy
\begin{align}
  \oxx = \omega_{X'X}\\
  0 \leq \oxx \leq 1\\
  \sum_{X'} \oxx = 1.
\end{align}
Inserting this form of $T$ into the detailed balance equation then gives
\begin{equation}
  \frac{A_{XX'}}{A_{X'X}} = \frac{\rho_{X'}}{\rho_{X}}.
\end{equation}
In the metropolis algorithm one chooses the acceptance probability as follows
Step 1: From state $X$ propose a new trial state $X'$ with probability given by $\oxx$. Then accept that state with probability 
\begin{equation}
  A_{XX'} = 
  \begin{cases}
    	1 &\trm{ if } \frac{\rho(X')}{\rho(X)} \geq 1\\
	\frac{\rho(X')}{\rho(X)}&\trm{ if } \frac{\rho(X')}{\rho(X)} < 1 .
  \end{cases}
\end{equation}
The trial probability is usually just a uniform distribution.
\section{Wolff on the 3DXY-model}
In the 3DXY-model, each spin is characterized by a single value, their angle $\alpha$.
We define the hamiltonian of the 3DXY-model as 
\begin{equation}
  H = \beta\sum_{<s_i,s_j>} \cos(\alpha_i - \alpha_j)
\end{equation}
where $<s_i,s_>$ denote that only sites $s_i$ and $s_j$ which are nearest neighours should be included.
The probability of adding spin to the cluster is
\begin{equation}
 P(\alpha_i,\alpha_j,\alpha_u) = 1 - \exp(2\beta \cos(\alpha_j - \alpha_u)\cos(\alpha_i - \alpha_u))
  \label{pflip}
\end{equation}
where $\bm{u}$ is the normal to the plane through which the spins are reflected when added.
When a spin is added to the cluster, it is reflected through a plane.
The Wolff-algorithm is defined as follows for the 3DXY-model:
\begin{enumerate}[(i)]
\item Select a starting spin with uniform probability. Select a random plane with normal $\bm{u}$, with the angle $\alpha_u$  to reflect spins through. Reflect the starting spin through the plane, taking it to 
$$\alpha \rightarrow R(\alpha,\alpha_u) = \pi + 2\alpha_u - \alpha$$

and mark it as part of cluster.

\item Add it's neighbours to a list of perimeter spins, to be tried for inclusion in the cluster.

\item Pick out any element in the list. Try to add it to the cluster with probability as defined above. If it is accepted, add it to the cluster. Add nearest neighbours that are not part of the cluster already to the perimeter list.

\item Repeat (iii) until the perimeter list is empty.
\end{enumerate}
For the Wolff-algorithm, to show that it satisfies detailed balance, condiser a 
state $X$ and a state $X'$ differing by a flip of the cluster $c$. 
The transition probabilities obey
\begin{align}
  \frac{T(X\rightarrow X')}{T(X'\rightarrow X)} &= \prod_{\langle s_i s_j \rangle \in \bm{\partial} c} \frac{ 1- P(R(\alpha_i,\alpha_u),\alpha_j,\alpha_u)}{1- P(R(\alpha'_i,\alpha_u),\alpha'_j,\alpha_u)}
\end{align}
where the product is over nearest neighbour bonds where $s_i \in x, s_j \notin c$.
Thus we have that $ R(\alpha_i,\alpha_u) = \alpha'_i $, $R(\alpha'_i) = \alpha_i$ and $\alpha_j = \alpha'_j$.
We get
\begin{align}
  \frac{T(X\rightarrow X')}{T(X'\rightarrow X)}&= \prod_{\langle s_i s_j \rangle \in \bm{\partial} c} \frac{ \exp\left\{2\beta\cos(R(\alpha_i,\alpha_u) - \alpha_u)\cos(\alpha_j - \alpha_u)\right\}}{ \exp\left\{2\beta\cos(R(\alpha'_i,\alpha_u) - \alpha_u)\cos(\alpha'_j -\alpha_u)\right\}} = \\
  &= \prod_{\langle s_i s_j \rangle \in \bm{\partial} c} \frac{ \exp\left\{2\beta\cos(\alpha'_i - \alpha_u)\cos(\alpha'_j - \alpha_u)\right\}}{ \exp\left\{2\beta\cos(\alpha_i,\alpha_u) - \alpha_u)\cos(\alpha_j -\alpha_u)\right\}} = \\
  &= \frac{\rho(X')}{\rho(X)},
\end{align}
and so, detailed balance is satisfied.

\section{Techniques to correct for scaling}
From the chapter of finite size scaling, we have that

\begin{align}
  b'(L) \equiv B(2L,t=0) - B(L,t=0) &= b_1(2^{-\omega} -1)L^{-\omega}\\
  b''(L)\equiv \frac{b'(2L)}{b'(L)} &= 2^{-\omega}\\
  \omega &= -\frac{\ln(b''(L))}{\ln(2)}.
\end{align}
Note that this only holds at the critical temperature, the $\textit{constants}$ $b_0, b_2$ are functions of temperature. Thus if we plot this function using at least four different system sizes, we can get several graphs that intersect at the critical temperature, and from that the scaling correction $\omega$ can be read of without the need for any form of parameter fitting. This depends on the assumption that the higher order corrections are sufficiently small, which they might not in fact be for the lowest system size we have simulated, $L=4$. This method also fails when $b'(L)$ and $b'(2L)$ differ in sign, since then $log(b''(l)$ will be undefined.
For such cases one can resort to methods of parameter fitting.
Consider the Superfluid density, which scales as 
\begin{align}
  r(L)&\equiv \rho_s(L,t=0) \cdot L = r_0 + r_1L^{-\omega_\rho}\\
  r(2L) - r(L) &= r_1(2^{-\omega_\rho} -1)L^{-\omega_\rho}
\end{align}


For the method using 2 system sizes, one can check by visual inspeciton each different value of omega to see for which omega the graphs intersect mostly at one point. One can also make a numerical check, by finding the intersecitons of all curves, and calculating  and plotting the standard deviation of the intersections for each omega.
The files containing the $L^{\omega}*(2L\rho_x(2L) - L\rho_x(L))$ vs $T$ is separately loaded for each calculated value of $\omega$, then intersections of these 3-5 lines are found. 
The intersection finder finds all intersections, 3 in the case of 3 lines, 6 for 4 lines 10 for 5 lines etc, and then calulates the average position of these intersections. The mean of the euclidean distances are then used as a measure of how close the intersections are. This value is then printed together with the value of omega, and finally plotted vs omega.
Hopefully the graph will have a clear minimum where the intersections are closest, thus determining the scaling correction omega.
For low systemsizes higher order corections can distort the found value, thus we visually inspect the graphs to judge how small system sizes should be included. 
\subsection{Constant subraction}
From the scaling hypothesis, 
\begin{align}
  b_L(t=0) &= \frac{\langle M^4 \rangle}{\langle M^2\rangle^2} = A + B\cdot L^{-\omega}\\
  v_L &\equiv b_L(t=0) -A = B\cdot L^{-\omega}.
\end{align}

This tells us that if we plot this new quantity $v_L$ on a log-log plot as a function of the system size $L$, it should be a constant functions with slope equal to negative omega.
If we instead plot $-\frac{\ln\left(\frac{v_{2L}}{v_L}\right)}{\ln(2)} = \omega$ versus the temperature, and for many combinations of L, we should get an intersection at omega, at Tc.
These methods requires that we know the constant $A$.
We can get a good starting estimation for A by extrapolation,
we plot the value of the binder parameter at the intersecion poins, versus the inverse of the system, and draw the line to $1/L = 0$.

\section{Parallell programming}
Simulating time required grows roughly as $L^3$ with system size, one way to speed up data gathering is to parallellize the simulations.
For the problems at hand, we chose to implement the trivial version of parallellization; run many simulations serially on many cores.
The Wolff algorithm can in fact be parallellized \cite{Kaupuzs2010}, but this did not come to our knowledge before a lot of code had been written, so we chose not to implement it. 

The main part of computations/simulations were performed on resources provided by the Swedish National Infrastructure for Computing (SNIC) at the PDC Center for High Performance Computing.
Some less demanding computations were performed on the Octopus-cluster at the Department of Theoretical Physics at KTH.

The parallellization was implemented using MPI, having a master process handling data output while the children ran simulations.
A process that is relatively slow to perform for a computer is to write data to disk. 
The startup cost of a file write is quite high, so if output is not printed as produced, but rather stored up in memory and then printed in larger chunks, cpu time can be saved. 
In effect, the larger the chucks the more efficent the program runs, but one is limited by the available amount of RAM when deciding chunk size.
Depending on computer system, the program may slow considerably or even crash should the available RAM be depleted during a run.
\section{Data analysis}
The analysis of the data from simulations are handled separately.
The implementations were written in Python, utilizing the NumPy and SciPy libraries. 
The figures where produced using the open source program Grace available at ``plasma-gate.weizmann.ac.il/Grace/''.

\section{Implementation}
The numerical method we used to perform the Monte-Carlo simulations was a Wolff algorithm written in C++. Since both the 3DXY-model and the 3D Ising model suffer from critcal slowing down, i.e. the equilibration time tends to infinity at the critical temperature, 
Both the 3DXY-model and the 3D Ising-model suffer from a phenomenon called critical slowing down. Near the transition temperature, the correlation length diverges, so thermal fluctuations propagate very long distances, and so the system takes a very long time to equilibrate to the equilibrum value.
One can wastly improve the quality-data versus cpu-time ratio by utilizing a global update algorithm such as the Wolff algorithm. 
\section{Error in estimation}
The Monte-Carlo method gives estimations of thermal averages,
\begin{equation}
  \langle A \rangle = \frac{1}{N}\sum_i A_i +\delta A.
  \label{}
\end{equation}
If the estimations $A_i$ are stochastically independent, the error can be estimated by
\begin{equation}
  \delta A \approx \frac{\sigma_A}{N-1}.
  \label{}
\end{equation}
This method works nicely for simple averages, no bias can arise and computation of the standard deviation is straight-forward and not computationally intense.
In our analysis we want to calculate functions of averages, such as the binder cumulant, heatcapacity, etc.
A way to do is to compute
$f(A) \approx \frac{1}{N}\sum_i f(A_i)$, and use the same method as above for error estimation.
But that is a bad way.
Functions of averages can produce bias, take for instance the fuction $f(X) = X^2$. Using an estimate  we get $ f(\bar{X} + \delta X) = (\bar{X})^2 + \bar{X}\delta X + (\delta X)^2$ where the last term introduces a positive bias.
To mitigate these biases one wants to first calculate the estimations of the averages as precisely as possible to minimize the error, as otherwise it can build up.
The above method of calculating the error estimate works as well, but a nicer way to do it is by using the jackknife-method.
One of it's nice features is that it can produce error-bars for functions of averages without having to calculate averages of functions of averages.
The method is a resampling method, one divides the data set into blocks, $x_\{i\}$, and then calculate the jackknife-average, defined as 
\begin{equation}
  x_i^J = \frac{1}{N-1}\sum_{j\neq i}x_j.
\end{equation}
The jackknife estimate of any function of averages is then defined as 
\begin{equation}
  f(X)\approx \bar{f^J}\equiv \frac{1}{N}\sum_{i=1}^{N} f(x_i^J).
  \label{}
\end{equation}
and the uncertainty in this estimate is defined as 
\begin{equation}
  \sigma_{f(\bar{X})} \equiv \sqrt{N-1}\sigma_{f^J}
\end{equation}
where
\begin{equation}
  \sigma^2_{f^J} = \bar{(f^J)^2} - (\bar{f^J})^2.
  \label{}
\end{equation}
The size of blocks can be chosen freely, but the result will depend on it. Chosing a smaller blocksize will increase the runtime of the algorithm.

